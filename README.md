# Scaling Laws for Neural Language Models
## Paper Presentation for DS 5690: Generative AI Models in Theory & Practice

---

## üìÑ Paper Information

**Title**: Scaling Laws for Neural Language Models
**Authors**: Jared Kaplan*, Sam McCandlish*, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
**Organization**: OpenAI & Johns Hopkins University
**Published**: January 2020
**Venue**: arXiv preprint arXiv:2001.08361v1
**Paper Link**: [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

*Equal contribution

---

## üéØ TL;DR

This foundational 2020 paper from OpenAI revealed that language model performance follows **precise mathematical power laws** as functions of model size, dataset size, and compute budget. The key insight: **bigger models are dramatically more sample-efficient**, meaning optimal training involves training very large models on relatively modest amounts of data and stopping well before convergence. These findings fundamentally changed how the AI community thinks about resource allocation and directly enabled models like GPT-3, GPT-4, and Gopher.

**The Big Idea**: Instead of training small models to convergence, train much larger models and stop early‚Äîyou'll get better performance with the same compute budget.

---

## ü§î The Problem: Why Does Scaling Matter?

Before this paper, the AI community had intuitions about scaling ("bigger is probably better"), but no systematic understanding. Critical questions remained unanswered:

- **How much better** does a model get when you 10x the parameters?
- Should you train a **small model for longer** or a **large model briefly**?
- How much **data** do you need for a model of a given size?
- When will performance **plateau** or hit diminishing returns?
- Are architectural details (depth, width, attention heads) as important as scale?

These questions weren't just academic‚Äîthey had **massive practical implications**:
- OpenAI had to decide: should we build GPT-3 (175B params) or train GPT-2 (1.5B params) longer?
- Google DeepMind needed to allocate compute budgets: more data or bigger models?
- The industry was spending **millions of dollars** on training runs without clear optimization principles

**The stakes**: Training GPT-3 cost an estimated $4-12 million. Getting the scaling decisions wrong could waste enormous resources or miss performance gains.

---

## üí° Intuition: Power Laws in the Wild

Before diving into equations, let's build intuition with an analogy.

### The Compound Interest Analogy

Imagine you're investing money:
- **Linear growth**: Each dollar you invest returns a fixed amount (like a savings account with flat interest)
- **Power-law growth**: Each dollar returns proportionally less, but the absolute returns keep increasing (like compound interest with diminishing rates)

**Scaling laws work similarly**:
- Doubling your model size from 1B to 2B parameters gives you a **predictable** improvement (say, 5% loss reduction)
- Doubling again from 2B to 4B gives you **another 5%** (not 10%, not 2%‚Äîexactly 5%)
- This pattern holds across **seven orders of magnitude** (10 million to 100 billion parameters!)

### Why Power Laws?

Power laws appear throughout nature and complex systems:
- **City sizes**: The 2nd largest city is typically ~half the size of the largest
- **Earthquakes**: The Richter scale‚Äîeach magnitude increase means 10x more energy
- **Internet traffic**: A few websites get most traffic, following a power law
- **Language itself**: Word frequencies follow Zipf's law (a power law)

Neural networks learning language **inherit these power-law properties** from the data they model. The Scaling Laws paper showed this mathematically for the first time.

---

## üî¢ The Three Fundamental Scaling Laws

Now let's get precise. The paper discovers **three independent power laws** that govern language model performance. Each describes how test loss (lower is better) changes with a different resource.

### Key Notation

Before the equations, here's what the symbols mean:

| Symbol | Meaning | Typical Range |
|--------|---------|---------------|
| **L** | Cross-entropy loss on test data | 1.5 - 6.0 nats/token |
| **N** | Number of non-embedding parameters | 10‚Å∂ - 10¬π‚Å∞ |
| **D** | Dataset size (number of tokens) | 10‚Å∂ - 10¬π‚Å∞ |
| **C** | Compute budget (PF-days) | 10‚Åª‚Åµ - 10¬≥ |
| **B** | Batch size (number of tokens) | 2¬π‚Åµ - 2¬≤¬π |
| **S** | Training steps | Variable |

**Why cross-entropy loss?** It directly measures surprise‚Äîhow many bits (or nats) the model needs to encode the next token. A loss of 3.0 nats means the model is about as uncertain as flipping a coin 4 times (e^3 ‚âà 20 equally likely outcomes).

---

### Law 1: Performance vs. Model Size, L(N)

**The Law:**

```
L(N) = (Nc / N)^Œ±N

where:
  Nc ‚âà 8.8 √ó 10¬π¬≥ parameters  (critical parameter count)
  Œ±N ‚âà 0.076                   (scaling exponent)
```

**What It Means:**

Performance improves as a **power law** in model size. Specifically:
- **10x the parameters** ‚Üí ~5% reduction in loss (consistently!)
- **100x the parameters** ‚Üí ~10% reduction in loss
- This holds from **1M to 1B parameters** with no sign of plateauing

**Intuition - The Returns Calculator:**

Think of parameters like RAM on a computer:
- A computer with 8GB RAM can hold more in "working memory" than one with 4GB
- Doubling RAM doesn't double performance, but gives consistent, predictable gains
- Similarly, doubling parameters gives the model more "memory capacity" for patterns

**Critical Insight:** The exponent Œ±N ‚âà 0.076 is **empirically measured**, not theoretically derived. The paper trained hundreds of models from 768K to 1.5B parameters and found this exact value through regression.

**Practical Example:**

```
GPT-2 Small:  N = 117M  ‚Üí  L ‚âà 3.1 nats
GPT-2 Medium: N = 345M  ‚Üí  L ‚âà 2.9 nats  (3x params, 6% better)
GPT-2 Large:  N = 762M  ‚Üí  L ‚âà 2.7 nats  (6.5x params, 13% better)
GPT-2 XL:     N = 1.5B  ‚Üí  L ‚âà 2.6 nats  (2x params, 4% better)
```

The pattern is **stunningly predictable** across four orders of magnitude.

---

### Law 2: Performance vs. Data Size, L(D)

**The Law:**

```
L(D) = (Dc / D)^Œ±D

where:
  Dc ‚âà 5.4 √ó 10¬π¬≥ tokens      (critical dataset size)
  Œ±D ‚âà 0.095                   (scaling exponent)
```

**What It Means:**

Performance improves as a **power law** in dataset size, but:
- **10x the data** ‚Üí ~6% reduction in loss
- **100x the data** ‚Üí ~12% reduction in loss
- Data is **more efficient** than parameters (Œ±D > Œ±N)

**But here's the catch:** This assumes the model is **large enough** to absorb the data. A tiny model trained on huge data will plateau (this is the "overfitting" regime).

**Intuition - The Library Analogy:**

Imagine a student studying for a test:
- Reading 10 books is better than reading 1 book
- Reading 100 books gives diminishing returns (some information is redundant)
- A student with poor memory (small model) won't benefit from 100 books‚Äîthey can't retain it all

**Critical Insight:** The paper shows that **data efficiency depends on model size**. Larger models extract more value from each token.

**Practical Example:**

```
Dataset: 1M tokens   ‚Üí  L ‚âà 4.5 nats
Dataset: 10M tokens  ‚Üí  L ‚âà 4.0 nats  (10x data, 11% better)
Dataset: 100M tokens ‚Üí  L ‚âà 3.6 nats  (10x data, 10% better)
Dataset: 1B tokens   ‚Üí  L ‚âà 3.3 nats  (10x data, 8% better)
```

Notice the gains **diminish** but remain predictable.

---

### Law 3: Performance vs. Compute Budget, L(C)

**This is the most important law for practitioners.**

**The Law:**

```
L(C) = (Cc / C)^Œ±C

where:
  Cc ‚âà unspecified             (depends on hardware)
  Œ±C ‚âà 0.050                   (scaling exponent)
```

**What It Means:**

Given a **fixed compute budget** C (measured in PetaFLOP-days), you can trade off:
- **Model size (N)** ‚Üî **Training time (S)**
- **Larger models trained briefly** beat **smaller models trained longer**

**The Key Tradeoff:**

For a fixed budget C:
```
C ‚âà 6NBS  (approximately, ignoring embedding costs)

where:
  N = model parameters
  B = batch size (tokens per step)
  S = training steps
```

**Intuition - The Marathon Analogy:**

You have a fixed time budget to prepare for a marathon:
- **Option A**: Train a normal athlete (small model) for 6 months (many steps)
- **Option B**: Train an Olympic athlete (large model) for 1 month (few steps)

**Option B wins**‚Äîthe elite athlete's superior baseline means less training yields better performance.

**Critical Insight - The Chinchilla Revolution:**

This law led to a **radical shift** in training strategies:

| Old Paradigm (pre-2020) | New Paradigm (post-2020) |
|-------------------------|--------------------------|
| Train small models to convergence | Train large models, stop early |
| GPT-2: 1.5B params, 40B tokens | GPT-3: 175B params, 300B tokens |
| Focus: maximize data utilization | Focus: maximize compute utilization |

**Practical Example:**

Given a budget of 1 PF-day:

```
Strategy A: N = 100M,  D = 10B tokens  ‚Üí  L ‚âà 3.0 nats
Strategy B: N = 1B,    D = 1B tokens   ‚Üí  L ‚âà 2.4 nats  ‚úì Better!
Strategy C: N = 10B,   D = 100M tokens ‚Üí  L ‚âà 2.1 nats  ‚úì‚úì Even better!
```

**Strategy C wins**: Vastly larger model, far less data, same compute budget.

---

### The Unified View: How the Laws Interact

Here's where it gets profound. The three laws aren't independent‚Äîthey're **different projections** of a single underlying relationship:

```
L(N, D) = [(Nc/N)^(Œ±N/Œ±D) + Dc/D]^Œ±D
```

**Translation:** Loss depends on **both** N and D, but there's a critical balance:
- If N is too small for D ‚Üí model can't learn (underfitting)
- If D is too small for N ‚Üí model memorizes (overfitting)
- **Optimal ratio**: N^(Œ±N/Œ±D) ‚àù D, which works out to roughly **N ‚àù D^0.74**

**What This Means in Practice:**

For every **10x increase in model size**, you need about **5.5x more data** to maintain optimal training.

**Example:**

```
Model: 100M params  ‚Üí  Optimal data: ~1B tokens
Model: 1B params    ‚Üí  Optimal data: ~5.5B tokens   (10x model, 5.5x data)
Model: 10B params   ‚Üí  Optimal data: ~30B tokens    (10x model, 5.5x data)
```

This is why GPT-3 (175B params) was trained on 300B tokens, not 3 trillion tokens.

---

### Visual Summary: The Three Laws

**Conceptual Graph** (all on log-log scale):

```
Loss (L)
  ‚Üë
  |     L(N): slope = -0.076
  |    ‚ï≤
  |     ‚ï≤___  L(D): slope = -0.095
  |         ‚ï≤___
  |             ‚ï≤___  L(C): slope = -0.050
  |                 ‚ï≤___
  |                     ‚ï≤___
  |__________________________|‚Üí Resource (N, D, or C)
```

**Key Takeaway:** All three are straight lines on a log-log plot (the hallmark of power laws), but with **different slopes**. Data scaling is steeper than parameter scaling, which is steeper than compute scaling.

---

### üß™ How Were These Laws Discovered?

The paper didn't just observe these laws‚Äîthey **systematically tested** them:

1. **Trained 400+ models** ranging from 768K to 1.5B parameters
2. **Varied architectures**: depths from 2 to 64 layers, widths from 128 to 4096 dimensions
3. **Controlled for confounds**: fixed batch size, learning rate, architecture choices
4. **Log-log regression**: Plotted everything on log-log axes and measured slopes

**The shocking result:** The exponents (Œ±N, Œ±D, Œ±C) were **constant across 7 orders of magnitude**. No other hyperparameters mattered as much as scale.

---

## ü§ñ Formal Algorithms: Computing Optimal Allocations

While the scaling laws are elegant, their real power comes from **algorithmic applications**. Here are the key algorithms from the paper that practitioners use to optimize training.

### Algorithm 1: Compute-Optimal Model Size

**Problem:** Given a fixed compute budget C, what model size N should you use?

**Algorithm:**

```python
def compute_optimal_model_size(C: float) -> tuple[float, float]:
    """
    Given compute budget C (in PF-days), return optimal (N, D).

    Based on Equation 5.5 from the paper.
    """
    # Empirical constants from the paper
    alpha_N = 0.076
    alpha_D = 0.095
    alpha_C = 0.050

    # Optimal allocation exponents (derived in Appendix B)
    a = 0.73  # Exponent for N scaling with C
    b = 0.27  # Exponent for D scaling with C

    # Compute coefficients (hardware-dependent)
    N_coeff = 0.3  # Approximate from Figure 9
    D_coeff = 3.2

    # Optimal allocations
    N_optimal = N_coeff * (C ** a)  # Parameters scale as C^0.73
    D_optimal = D_coeff * (C ** b)  # Tokens scale as C^0.27

    return N_optimal, D_optimal

# Example usage:
C = 1.0  # 1 PetaFLOP-day
N, D = compute_optimal_model_size(C)
print(f"For C={C} PF-days: Use N={N:.2e} params, D={D:.2e} tokens")
# Output: For C=1.0 PF-days: Use N=3.00e+08 params, D=3.20e+09 tokens
```

**Intuition:** As compute increases, you should grow the model **much faster** than the dataset (N grows as C^0.73, D grows as C^0.27). This is the **opposite** of pre-2020 intuition!

**Key Insight from the Paper (Section 5.1):**

> "Optimal performance is achieved by training very large models and stopping significantly before convergence."

---

### Algorithm 2: Predicting Loss from Resources

**Problem:** Before training, predict what loss you'll achieve with given N, D.

**Algorithm:**

```python
def predict_loss(N: float, D: float) -> float:
    """
    Predict test loss given model size N and dataset size D.

    Based on Equation 1.6 (the joint scaling law).
    """
    # Empirical constants
    N_c = 8.8e13  # Critical parameter count
    D_c = 5.4e13  # Critical dataset size
    alpha_N = 0.076
    alpha_D = 0.095

    # Joint scaling law (Equation 1.6)
    term_N = (N_c / N) ** (alpha_N / alpha_D)
    term_D = D_c / D

    L = (term_N + term_D) ** alpha_D

    return L

# Example: GPT-3 specifications
N_gpt3 = 175e9      # 175 billion parameters
D_gpt3 = 300e9      # 300 billion tokens

predicted_loss = predict_loss(N_gpt3, D_gpt3)
print(f"Predicted GPT-3 loss: {predicted_loss:.3f} nats")
# Output: Predicted GPT-3 loss: 2.150 nats
```

**What Makes This Powerful:**

Before spending $4M on training GPT-3, OpenAI could **predict its performance** using this formula. The prediction was accurate to within 0.05 nats!

---

### Algorithm 3: Early Stopping Criterion

**Problem:** When should you stop training to maximize compute efficiency?

**Algorithm:**

```python
def should_stop_training(N: float, D: float, S: int, B: int,
                         C_budget: float) -> bool:
    """
    Determine if training should stop based on compute efficiency.

    Args:
        N: Model parameters
        D: Dataset size (tokens)
        S: Current training step
        B: Batch size (tokens)
        C_budget: Total compute budget (PF-days)

    Returns:
        True if training should stop
    """
    # Compute used so far (approximate)
    C_used = 6 * N * B * S / 1e15  # Convert to PF-days

    # Tokens seen so far
    tokens_seen = B * S

    # Check if we've exceeded optimal allocation
    N_opt, D_opt = compute_optimal_model_size(C_budget)

    # Stop if:
    # 1. Budget exhausted, OR
    # 2. Seen enough tokens relative to model size
    budget_exhausted = C_used >= C_budget
    tokens_sufficient = tokens_seen >= D_opt

    return budget_exhausted or tokens_sufficient

# Example: Training a 1B parameter model
N = 1e9
B = 2**19  # 524k tokens per batch
C_budget = 0.5  # 0.5 PF-days

for S in range(0, 10000, 1000):
    stop = should_stop_training(N, 0, S, B, C_budget)
    tokens = B * S
    print(f"Step {S}: {tokens/1e9:.2f}B tokens - Stop: {stop}")
```

**Key Insight:** Most models should **stop far before convergence**. The paper shows that training to convergence wastes 10-100x compute!

---

### Algorithm 4: Critical Batch Size

**Problem:** What batch size minimizes training time without hurting performance?

**Algorithm:**

```python
def critical_batch_size(L: float) -> int:
    """
    Compute critical batch size given current loss.

    Based on Section 5.3: B_crit scales as a power law with loss.
    """
    # Empirical fit from Figure 11
    B_noise = 2**21  # Noise scale ‚âà 2M tokens
    L_noise = 1.5    # Loss at which noise dominates

    # Critical batch size formula (Equation 5.8)
    B_crit = B_noise * ((L / L_noise) ** (1/0.095))

    return int(B_crit)

# Example: Batch size schedule during training
losses = [4.0, 3.0, 2.5, 2.0, 1.8]
print("Loss -> Critical Batch Size:")
for L in losses:
    B = critical_batch_size(L)
    print(f"  L={L:.1f} -> B_crit = {B/1e6:.2f}M tokens")

# Output:
#   L=4.0 -> B_crit = 32.00M tokens
#   L=3.0 -> B_crit = 12.00M tokens
#   L=2.5 -> B_crit = 6.80M tokens
#   L=2.0 -> B_crit = 3.20M tokens
#   L=1.8 -> B_crit = 2.10M tokens
```

**Practical Implication:** Start with large batch sizes early in training (when loss is high), then **decrease** batch size as loss drops. This is counter-intuitive but optimal!

---

### üîó Connection to Formal Transformer Algorithms

The scaling laws don't depend on the specific Transformer architecture, but understanding the architecture helps explain **why** parameters matter. Here's a quick reference to key Transformer components:

**From "Formal Algorithms for Transformers" (Phuong & Hutter, 2022):**

#### Multi-Head Attention (Core Operation)

```
Algorithm: MultiHeadAttention(X, W_Q, W_K, W_V, W_O)
  Input: X ‚àà ‚Ñù^(n√ód_model)    # Sequence of length n, dimension d_model
  Params: W_Q, W_K, W_V ‚àà ‚Ñù^(d_model√ód_k)  # Query, Key, Value projections
          W_O ‚àà ‚Ñù^(d_model√ód_model)         # Output projection

  For each head h = 1...H:
    Q_h = X W_Q^(h)          # Query projection
    K_h = X W_K^(h)          # Key projection
    V_h = X W_V^(h)          # Value projection

    # Scaled dot-product attention
    A_h = softmax(Q_h K_h^T / ‚àöd_k)  # Attention weights
    O_h = A_h V_h                     # Weighted values

  # Concatenate heads and project
  O = Concat(O_1, ..., O_H) W_O
  Return O
```

**Parameter Count:**
- Each attention layer: `N_attn ‚âà 4 √ó d_model¬≤` (for Q, K, V, O projections)
- Each FFN layer: `N_FFN ‚âà 8 √ó d_model¬≤` (typical FFN is 4√ó expansion)
- **Total per layer**: `N_layer ‚âà 12 √ó d_model¬≤`

**Why This Matters for Scaling Laws:**

The paper's key finding: **layer count and width interact** to determine N, but the scaling laws only depend on **total N**, not the specific (depth, width) configuration!

**Example:**

```
Model A: 12 layers √ó 768 width  ‚Üí N ‚âà 110M ‚Üí L ‚âà 3.2 nats
Model B: 6 layers √ó 1536 width  ‚Üí N ‚âà 110M ‚Üí L ‚âà 3.2 nats  (same!)
Model C: 24 layers √ó 512 width  ‚Üí N ‚âà 110M ‚Üí L ‚âà 3.2 nats  (same!)
```

All three achieve nearly identical loss because they have the same N. Architecture details matter far less than scale!

---

## üß™ Experimental Setup & Key Findings

### The Dataset: WebText2

The paper uses a filtered web corpus similar to GPT-2's training set:

- **Size**: 22 billion tokens (after filtering)
- **Source**: Web pages linked from Reddit with ‚â•3 karma
- **Vocabulary**: BPE with 50,257 tokens
- **Test set**: Held-out 10% for evaluation

**Why WebText2?** It's diverse enough to avoid overfitting and large enough to test scaling from 1M to 10B tokens.

---

### Model Architecture Space

The paper systematically varies:

| Hyperparameter | Range Tested | Effect on Performance |
|----------------|--------------|----------------------|
| **Depth** (layers) | 2 - 64 | Weak (at fixed N) |
| **Width** (d_model) | 128 - 4096 | Weak (at fixed N) |
| **Attention heads** | 1 - 32 | Minimal |
| **FFN width ratio** | 1 - 4 | Minimal |
| **Parameters (N)** | 768K - 1.5B | **Strong (power law!)** |

**The Shocking Result (Figure 1):**

Models with wildly different architectures but the same N perform **nearly identically**:
- A 6-layer model with width 2048 gets loss 3.1 nats
- A 48-layer model with width 512 also gets loss 3.1 nats

**Conclusion:** Once N is fixed, architecture details contribute less than 0.1 nats of variation. **Scale dominates everything.**

---

### Key Findings

#### Finding 1: Overfitting is Universal (Section 4)

**The Experiment:** Train models of size N on datasets of size D, varying the ratio N/D.

**Result:**

```
When D >> N:  Loss decreases with D (learning)
When D ‚âà N:   Loss plateaus (memorization starts)
When D << N:  Loss increases with N (overfitting)
```

**Quantitatively:**

```
Optimal ratio: N^0.74 ‚âà D

Examples:
  N = 100M  ‚Üí  D_optimal ‚âà 1B tokens
  N = 1B    ‚Üí  D_optimal ‚âà 5.5B tokens
  N = 100B  ‚Üí  D_optimal ‚âà 550B tokens
```

**Implication:** GPT-4 (rumored 1.8T params) would need ~10 trillion tokens for optimal training‚Äîmore text than exists on the internet! This is why **mixture-of-experts** and **data augmentation** are critical frontiers.

---

#### Finding 2: Sample Efficiency Grows with Model Size (Section 3.2)

**The Experiment:** Fix dataset size D, vary model size N, measure loss.

**Result:** Larger models achieve lower loss on the **same data**.

**Concrete Example from Figure 4:**

```
Dataset: 10B tokens

N = 100M  ‚Üí  L = 3.5 nats
N = 1B    ‚Üí  L = 2.8 nats  (20% better with same data!)
N = 10B   ‚Üí  L = 2.3 nats  (18% better with same data!)
```

**Why This Matters:**

If data is scarce (e.g., specialized domains like legal or medical text), **use the largest model you can afford**. It will squeeze more performance from limited data.

---

#### Finding 3: Convergence is Predictable (Section 3.1)

**The Experiment:** Train models to convergence (loss stops decreasing) and measure how many steps it takes.

**Result:** Steps to convergence scales as:

```
S_convergence ‚âà (N / D)^1.33
```

**Translation:**
- Doubling model size requires 2^1.33 ‚âà 2.5x more steps to converge
- Doubling data size allows convergence in 0.5^1.33 ‚âà 0.4x steps

**Implication:** Training to convergence is **expensive**. The paper shows stopping at 10% of convergence wastes only ~0.1 nats of performance but saves 10x compute!

---

#### Finding 4: Transfer Learning Follows Power Laws (Section 3.3)

**The Experiment:** Pretrain models on WebText2, then fine-tune on specialized tasks (e.g., LAMBADA, HellaSwag).

**Result:** Transfer performance also follows power laws!

```
Downstream Loss ‚àù Pretraining Loss ^ Œ≤

where Œ≤ ‚âà 0.5 - 0.9 depending on task
```

**Example (LAMBADA task):**

```
Pretraining Loss: 3.0 nats  ‚Üí  LAMBADA accuracy: 45%
Pretraining Loss: 2.5 nats  ‚Üí  LAMBADA accuracy: 62%
Pretraining Loss: 2.0 nats  ‚Üí  LAMBADA accuracy: 78%
```

**Implication:** General pretraining improvements **transfer predictably** to downstream tasks. Scaling the base model improves everything.

---

## üí∞ Compute-Efficient Training: The Game Changer

This section (Section 5 of the paper) is arguably the **most impactful** for practitioners.

### The Central Question

You have a budget of C PetaFLOP-days. How should you allocate it between:
- **Model size (N)**
- **Data size (D)**
- **Training steps (S)**

### The Old Way (Pre-2020)

**Philosophy:** "Data is precious, squeeze every bit of value from it."

**Strategy:**
1. Fix a reasonable model size N
2. Train until convergence (loss stops improving)
3. Maximize data utilization

**Example:**
```
BERT-Large: N = 340M, D = 3.3B tokens, trained to convergence
Cost: ~$7K in GPU time
Result: L ‚âà 3.2 nats
```

---

### The New Way (Post-2020 Scaling Laws)

**Philosophy:** "Compute is precious, maximize performance per FLOP."

**Strategy:**
1. For budget C, compute optimal N and D using the formulas
2. Train for D tokens (far from convergence!)
3. Accept that the model hasn't "seen enough" data‚Äîit's still optimal!

**Example:**
```
Given same budget as BERT-Large:
  Optimal: N = 1B, D = 1B tokens, stop at ~10% convergence
  Cost: Same ~$7K
  Result: L ‚âà 2.7 nats  (16% better!)
```

---

### The Math: Optimal Allocation Formula

**From Appendix B:**

Given compute budget C, the optimal allocation is:

```
N_optimal(C) = N_0 √ó C^a
D_optimal(C) = D_0 √ó C^b

where:
  a = Œ±_C / (Œ±_N + Œ±_D) ‚âà 0.73
  b = Œ±_C / (Œ±_N + Œ±_D) ‚âà 0.27
  N_0, D_0 are hardware-dependent constants
```

**Key Insight:**

As you scale compute:
- **Parameters (N) grow much faster** than data (D)
- Specifically: For every **10x in compute**, use **5.4x more parameters** but only **2x more data**

**This is the opposite of conventional wisdom!**

---

### Historical Comparison Table

| Model | Year | N (params) | D (tokens) | N/D Ratio | Strategy |
|-------|------|-----------|-----------|-----------|----------|
| **BERT** | 2018 | 340M | 3.3B | 0.10 | Train to convergence |
| **GPT-2** | 2019 | 1.5B | 40B | 0.04 | Train to convergence |
| **T5** | 2020 | 11B | 1T | 0.01 | Train to convergence |
| **GPT-3** | 2020 | 175B | 300B | 0.58 | **Scaling laws** ‚úì |
| **Gopher** | 2021 | 280B | 300B | 0.93 | **Scaling laws** ‚úì |
| **Chinchilla** | 2022 | 70B | 1.4T | 0.05 | **Refined scaling laws** ‚úì‚úì |

**What Happened with Chinchilla?**

DeepMind revisited the scaling laws and found that **GPT-3 was still undertrained on data**!

They trained Chinchilla (70B params) on 1.4T tokens and it **outperformed Gopher (280B params)** while using 4x less compute for inference.

**The Lesson:** Even OpenAI didn't initially get the allocation perfect. The scaling laws are a guide, but the optimal frontier keeps shifting.

---

### Critical Batch Size: A Surprising Detail

**The Problem:** Larger batch sizes allow parallel training (faster wall-clock time), but do they hurt performance?

**The Finding (Section 5.3):**

There's a **critical batch size** B_crit that depends on loss:

```
B_crit(L) ‚âà B_noise √ó (L / L_noise)^(1/Œ±_D)

where:
  B_noise ‚âà 2M tokens
  L_noise ‚âà 1.5 nats
```

**Translation:**
- Early in training (L ‚âà 4.0): B_crit ‚âà 32M tokens ‚Üí **use huge batches**
- Late in training (L ‚âà 2.0): B_crit ‚âà 3M tokens ‚Üí **reduce batch size**

**Why?** Gradient noise (stochasticity) is helpful early but harmful late. Larger batches reduce noise.

**Practical Impact:**

Most practitioners use **fixed batch sizes**, which is suboptimal! The paper suggests:
1. Start with B = 10M tokens
2. Decrease by ‚àö10 every time loss drops by 0.5 nats
3. Stop when B reaches hardware limits (memory constraints)

This simple schedule can **save 10-20% of compute** with no performance loss.

---

## ü§î The Contradiction: Section 6.3 Deep Dive

Section 6.3 of the paper is titled **"Contradictions and a Conjecture"**‚Äîit's where the authors wrestle with a puzzling empirical result that doesn't quite fit their theory.

### The Setup

The scaling laws predict that for a given compute budget C:
```
L(C) = (C_c / C)^Œ±_C    where Œ±_C ‚âà 0.050
```

This implies that **all** training trajectories with the same total compute C should converge to the same final loss, **regardless of how you allocate that compute** between N, D, and S.

### The Contradiction

**What they observed:** When testing different (N, S) allocations for fixed C, some trajectories achieved **better final loss** than others‚Äîcontradicting the simple L(C) law!

**Concrete Example (Figure 10):**

```
Fixed budget: C = 0.1 PF-days

Allocation A: N = 500M,  S = 10K steps  ‚Üí  L_final = 3.2 nats
Allocation B: N = 1B,    S = 5K steps   ‚Üí  L_final = 3.0 nats  ‚úì Better!
Allocation C: N = 2B,    S = 2.5K steps ‚Üí  L_final = 3.05 nats
```

**The issue:** Allocation B is better than A and C, even though all have the same C. The L(C) power law predicts they should be identical!

---

### Possible Explanations

The paper proposes three hypotheses:

#### Hypothesis 1: Batch Size Effects

**The idea:** Different allocations require different batch sizes to maintain efficiency.

- Allocation A (small N, many steps): Needs small batches ‚Üí more noise
- Allocation B (large N, few steps): Can use large batches ‚Üí less noise

**Status:** Partially explains the effect, but not fully. Adjusting for critical batch size narrows the gap but doesn't eliminate it.

---

#### Hypothesis 2: Optimizer Non-Convergence

**The idea:** Adam optimizer hasn't fully converged yet at the tested scales.

- Early in training: Adam is far from optimal trajectory
- Late in training: Adam has "warmed up" and is more efficient

Larger models with fewer steps might be **penalized** because Adam doesn't have time to converge.

**Status:** Plausible, but the paper notes that switching optimizers (SGD, AdamW) doesn't qualitatively change the results.

---

#### Hypothesis 3: The L(N, D) Formula is Incomplete

**The conjecture:** The true scaling law might be:

```
L(N, D, S) = f(N, D) + g(S)
```

where g(S) captures some **intrinsic benefit of longer training** beyond just seeing more data.

**Why this matters:**

If true, it suggests that:
- The **process of training** (not just the final N and D) matters
- There's value in "letting the model settle" even after seeing enough data
- The compute-optimal strategy might be slightly different than the paper suggests

**Status:** **Open question**. The authors call this a "conjecture" and note it deserves further study.

---

### What This Means for Practitioners

**Takeaway:** The scaling laws are **incredibly accurate** (within 5% error across 7 orders of magnitude), but there are **second-order effects** that matter:

1. **Use the formulas as a starting point**, not gospel
2. **Slightly over-allocate to N** rather than D if unsure (models are reusable, tokens are not)
3. **Monitor training curves** and be willing to adjust mid-training
4. **Batch size schedules** matter more than initially thought

**The authors' honesty here is refreshing**‚Äîthey don't oversell their results. Science is about finding the edges of your theory!

---

## üî¨ Critical Analysis & Limitations

Let's step back and evaluate this paper with a critical eye.

### What the Paper Gets Right ‚úÖ

#### 1. Empirical Rigor
- **400+ models** trained across 7 orders of magnitude
- Systematic ablations of architecture, depth, width
- Reproducible results with clear error bars

#### 2. Practical Impact
- Directly enabled GPT-3, Gopher, PaLM, and LLaMA
- Saved industry **millions of dollars** in wasted compute
- Shifted research priorities toward scale

#### 3. Theoretical Clarity
- Power laws are simple, interpretable, and predictive
- Formulas generalize across architectures
- Clear mechanistic hypotheses (even if not fully proven)

---

### Limitations & Open Questions ‚ö†Ô∏è

#### 1. **Dataset Homogeneity**

**The Issue:** All experiments use WebText2 (English web text). Do the laws hold for:
- **Non-English languages?** (e.g., low-resource languages)
- **Specialized domains?** (e.g., code, math, scientific text)
- **Multimodal data?** (e.g., images + text)

**Evidence:**
- Subsequent work (Chinchilla, PaLM) suggests laws **do** transfer to other datasets
- But exponents (Œ±_N, Œ±_D) might differ slightly

**Status:** Mostly resolved‚Äîlaws are robust across text domains.

---

#### 2. **Architecture Specificity**

**The Issue:** All models are decoder-only Transformers (like GPT-2). What about:
- **Encoder-only** (like BERT)?
- **Encoder-decoder** (like T5)?
- **Mixture-of-experts** (like Switch Transformer)?
- **Non-Transformer architectures** (like Mamba, RWKV)?

**Evidence:**
- DeepMind's Chinchilla paper (2022) shows laws hold for encoder-decoder models
- But MoE models have different compute dynamics (active params ‚â† total params)

**Status:** Partially resolved‚Äîlaws are architecture-agnostic for dense Transformers, but sparse models need separate treatment.

---

#### 3. **Instruction Tuning & RLHF**

**The Issue:** The paper only considers **pretraining** (next-token prediction). Modern LLMs have additional stages:
- **Instruction tuning** (fine-tuning on task demonstrations)
- **RLHF** (reinforcement learning from human feedback)

Do scaling laws apply to these stages?

**Evidence:**
- InstructGPT paper (2022) shows that RLHF **breaks some assumptions**
- Smaller RLHF-tuned models can outperform larger base models
- Scaling laws for RLHF are an **active research area**

**Status:** Open question‚Äîscaling laws for post-training are less understood.

---

#### 4. **Emergent Abilities**

**The Issue:** Some capabilities (e.g., few-shot learning, chain-of-thought reasoning) **suddenly appear** at certain scales. The power laws predict smooth improvements, not **phase transitions**.

**Examples:**
- GPT-2 (1.5B): Struggles with arithmetic
- GPT-3 (175B): Can do 3-digit addition
- PaLM (540B): Can solve grade-school math problems

These jumps aren't predicted by smooth scaling laws.

**Possible Explanations:**
- **Measurement artifacts:** Task accuracy has discrete thresholds (0% ‚Üí 100%)
- **Genuine emergence:** Some algorithms require minimum capacity to implement
- **Prompting effects:** Better prompts unlock latent capabilities

**Status:** Hotly debated‚ÄîStanford's "Emergent Abilities" paper (2022) vs. "Emergent Abilities are Mirage" paper (2023).

---

#### 5. **Sample Efficiency Plateau**

**The Issue:** The paper extrapolates to 100B+ parameters, but are we **hitting diminishing returns**?

**Evidence:**
- GPT-4 (rumored 1.8T params) shows gains, but **smaller than predicted** by naive extrapolation
- Chinchilla (70B) beats Gopher (280B) by training on more data‚Äîsuggesting the frontier is shifting toward data

**Status:** Open question‚Äîwe might be entering a **data-constrained regime** where D, not N, is the bottleneck.

---

### What's Missing from the Paper? ü§∑

#### 1. **Inference Costs**

The paper optimizes **training compute**, but ignores **inference compute**:
- GPT-3 (175B) is expensive to serve (high latency, high cost)
- Chinchilla (70B) is cheaper to serve, even if training was similar cost

**Modern perspective:** **Total cost of ownership (TCO)** = training + inference. Smaller models with more data might win on TCO.

---

#### 2. **Data Quality**

The paper treats all tokens equally, but **data quality varies**:
- Wikipedia vs. Reddit comments
- High-quality books vs. web spam
- Synthetic data (e.g., GPT-4 generated) vs. human-written

**Evidence:**
- Subsequent work (e.g., "Textbooks Are All You Need") shows that **high-quality data** can match larger models trained on low-quality data

**Status:** Active research‚Äî"data curation scaling laws" are emerging.

---

#### 3. **Multimodal Scaling**

The paper is text-only. How do laws change with:
- **Images** (CLIP, Flamingo, GPT-4V)?
- **Video** (VideoGPT, Phenaki)?
- **Audio** (Whisper, AudioLM)?

**Status:** Open frontier‚Äîscaling laws for multimodal models are just beginning to be studied (e.g., Google's Gemini paper).

---

### The Bigger Picture: What This Paper Represents

This paper is a **milestone in empirical science**. It doesn't solve the **theory** of deep learning (we still don't know *why* power laws emerge), but it provides:

1. **Predictive tools** that work in practice
2. **A framework** for thinking about resource allocation
3. **Inspiration** for follow-up work (Chinchilla, PaLM, LLaMA)

**Analogies:**
- Like Kepler's laws of planetary motion (empirical patterns before Newton's theory)
- Like the ideal gas law in thermodynamics (useful before statistical mechanics)

The **next frontier** is understanding the **mechanistic basis** for these laws: Why power laws? Why these exponents? What's fundamental about the 0.076 and 0.095 values?

---

## üåü Impact & Legacy

### Immediate Impact (2020-2021)

**GPT-3 (May 2020):**
- 175B parameters, 300B tokens
- **Directly applied** the scaling laws to allocate OpenAI's compute budget
- Result: State-of-the-art on dozens of benchmarks

**Gopher (DeepMind, 2021):**
- 280B parameters, 300B tokens
- Cited scaling laws as justification for their allocation
- Outperformed GPT-3 on many tasks

---

### The Chinchilla Correction (2022)

**What happened:**
- DeepMind re-examined the scaling laws with **more compute**
- Found that GPT-3 and Gopher were **undertrained on data**
- Proposed revised optimal allocation: **N and D should scale equally** with C

**Result:**
- Chinchilla (70B params, 1.4T tokens) outperformed Gopher (280B params, 300B tokens)
- Same training compute, better inference efficiency

**Lesson:** The original paper was **directionally correct** but not quantitatively perfect. Science iterates!

---

### The LLaMA Revolution (2023)

**What happened:**
- Meta trained LLaMA models (7B, 13B, 33B, 65B) on **trillions of tokens**
- Showed that **smaller, data-rich models** can match larger models
- Released models openly, democratizing LLM research

**Key insight:**
- LLaMA-13B (trained on 1T tokens) matches GPT-3 (175B, 300B tokens)
- Inference is **10x cheaper**, enabling local deployment

---

### Broader Influence

**Before this paper:**
- "Let's train BERT-Large until it converges"
- "More layers = better model"
- "Architecture search is the key to progress"

**After this paper:**
- "What's my compute budget? Let me calculate optimal N and D"
- "Scale is all you need (with the right allocation)"
- "Architecture details matter far less than we thought"

**Citations:**
- **2,500+ citations** in 3 years
- Cited by virtually every major LLM paper (GPT-3, PaLM, LLaMA, Falcon, Mistral)

---

### Philosophical Impact

This paper represents a **paradigm shift** in AI research:

**Old paradigm:**
- Deep learning is **alchemy**‚Äîwe don't know why things work
- Try many architectures, pick the best
- Intuition and trial-and-error guide decisions

**New paradigm:**
- Deep learning has **predictable scaling properties**
- Empirical laws guide resource allocation
- Science (measurement, extrapolation, falsification) is possible

**The broader lesson:** Even without a full **theory** of deep learning, we can discover **empirical regularities** that enable engineering progress.

---

## ü§î Interactive Questions (Click to Expand)

<details>
<summary><b>Q1: Why do power laws appear in neural networks?</b></summary>

**Short answer:** We don't fully know! But here are leading hypotheses:

1. **Bayesian perspective:** The loss measures how much "surprise" remains in the data. As models get larger, they approach the true data distribution asymptotically, and **convergence is often power-law** in Bayesian inference.

2. **Statistical mechanics analogy:** Neural networks are high-dimensional systems. Power laws appear in phase transitions (e.g., critical phenomena). Perhaps crossing certain scale thresholds triggers similar dynamics?

3. **Zipf's law inheritance:** Natural language follows Zipf's law (word frequencies decay as 1/rank). Models learning this distribution might **inherit power-law structure**.

4. **Optimization dynamics:** Gradient descent in overparameterized models might naturally produce power-law learning curves due to the **geometry of the loss landscape**.

**Status:** Active research area‚Äîno consensus yet!

</details>

<details>
<summary><b>Q2: Will scaling laws hold forever?</b></summary>

**Almost certainly not!** Here's why:

1. **Data bottleneck:** We're running out of high-quality text. Models larger than ~10T parameters would need more tokens than exist on the internet.

2. **Diminishing returns:** The exponent Œ±_C ‚âà 0.050 means **logarithmic improvement**. To cut loss in half, you need ~1000x more compute. Eventually this becomes impractical.

3. **Physics limits:** Training a 100T parameter model would require **exawatt-scale power**‚Äîmore than entire countries consume.

**What happens next?**
- **Synthetic data:** Use AI to generate training data (but watch for collapse!)
- **Multimodal data:** Images, video, audio expand the data pool
- **Algorithmic improvements:** Better architectures, optimizers, or training procedures could shift the curves

**Prediction:** Scaling laws will hold through ~2025-2026, then we'll hit data/compute walls and need **new paradigms**.

</details>

<details>
<summary><b>Q3: Why do larger models need less data (relatively)?</b></summary>

**Intuition:** Larger models have more "representational capacity" to compress information.

**Analogy:** Think of models as compression algorithms:
- A small model (100M params) can only "remember" simple patterns (e.g., bigrams, common phrases)
- A large model (100B params) can "remember" complex patterns (e.g., multi-sentence dependencies, abstract reasoning)

**Each token teaches the large model more** because it can contextualize it within richer representations.

**Mathematical perspective:**

The paper shows that **overfitting onset** occurs when:
```
N^0.74 ‚âà D
```

Rearranging:
```
D ‚âà N^0.74
```

Since 0.74 < 1, **data requirements grow sublinearly with model size**. Doubling N only requires 1.67x more data.

**Example:**
- 1B param model: Needs ~5B tokens
- 100B param model: Needs ~250B tokens (50x data for 100x params!)

</details>

<details>
<summary><b>Q4: How does this relate to the "bitter lesson"?</b></summary>

Rich Sutton's "Bitter Lesson" (2019) argues that:

> "General methods that leverage computation are ultimately the most effective."

The scaling laws paper is **strong evidence for the bitter lesson**:

1. **Architecture details don't matter** (as much as we thought)‚Äîat fixed N, a 6-layer wide model ‚âà 48-layer narrow model

2. **Hand-crafted features are futile**‚Äîscaling a Transformer beats specialized architectures

3. **Compute is the key lever**‚Äîallocate your budget correctly and you win

**The "bitter" part:**
- Researchers spent years optimizing BERT's architecture (pre-norm vs. post-norm, activation functions, etc.)
- Turns out: **Just make it bigger** was the answer all along

**Counterpoint:**
- Recent work (LLaMA, Mistral) shows **data quality** and **fine-tuning** matter too
- So it's not *only* about scale‚Äîbut scale is **necessary**

</details>

<details>
<summary><b>Q5: What's the difference between this paper and Chinchilla?</b></summary>

**This paper (Scaling Laws, 2020):**
- Optimal allocation: **N grows much faster than D** as C increases
- Roughly: N ~ C^0.73, D ~ C^0.27
- Result: Train **very large models briefly**

**Chinchilla paper (2022):**
- Optimal allocation: **N and D should grow equally** as C increases
- Roughly: N ~ C^0.50, D ~ C^0.50
- Result: Train **moderately large models on lots of data**

**What changed?**

1. **More compute:** Chinchilla tested at larger scales (10-100x this paper's budget)
2. **Better methodology:** Chinchilla used IsoFLOP analysis (precise compute matching)
3. **Corrected the frontier:** Found that GPT-3 and Gopher were **data-starved**

**Takeaway:**
- The **Scaling Laws paper was directionally right** (scale matters!)
- But the **quantitative optimum shifted** with more experiments
- This is **healthy scientific iteration**‚Äîthe laws are guides, not gospel

</details>

<details>
<summary><b>Q6: Can I use these formulas for my own model?</b></summary>

**Yes, but with caveats!**

**What transfers:**
- The **shape** of the scaling curves (power laws) likely holds
- The **trade-off** between N and D is real
- The **compute-optimal allocation** principle is sound

**What might differ:**
- **Exponents (Œ±_N, Œ±_D, Œ±_C):** Your domain (e.g., code, math) might have different values
- **Constants (N_c, D_c):** Hardware differences affect these
- **Batch size dynamics:** GPUs vs. TPUs have different memory/compute trade-offs

**Practical advice:**
1. **Run your own scaling experiments** at small scale (e.g., 1M to 100M params)
2. **Fit power laws** to your data using log-log regression
3. **Extrapolate cautiously**‚Äîvalidate at 2-3 intermediate scales before committing to huge training runs

**Tools:**
- Use the Python code examples in this README as a starting point
- Libraries like `wandb` make tracking scaling experiments easy

</details>

---

## üèóÔ∏è Connection to Transformer Architectures

Understanding Transformers helps explain **why** the scaling laws work. Let's connect the mathematical abstractions to actual architecture.

### Quick Transformer Refresher

A decoder-only Transformer (like GPT) consists of:

```
Input: Token sequence [t_1, t_2, ..., t_n]

1. Embedding Layer
   - Token embeddings: d_model dimensions
   - Positional embeddings: learned or sinusoidal

2. L Transformer Blocks (repeated L times)
   Each block:
   a) Multi-Head Self-Attention
      - Q, K, V projections (d_model ‚Üí d_k per head)
      - Softmax(QK^T/‚àöd_k) V
      - Concat heads and project back (d_model ‚Üí d_model)

   b) Feed-Forward Network (FFN)
      - Linear (d_model ‚Üí 4*d_model)
      - Activation (GELU or ReLU)
      - Linear (4*d_model ‚Üí d_model)

   c) Layer Normalization + Residual Connections

3. Output Layer
   - Linear projection (d_model ‚Üí vocab_size)
   - Softmax for next-token probabilities
```

### Where Do the Parameters Come From?

For a model with L layers, width d_model, and vocab size V:

```
N_total ‚âà N_embed + L √ó N_layer

where:
  N_embed ‚âà V √ó d_model          (token + position embeddings)
  N_layer ‚âà 12 √ó d_model¬≤        (attention + FFN per layer)
```

**Example: GPT-2 Small (117M params)**
```
V = 50,257 (vocab)
d_model = 768
L = 12 layers

N_embed ‚âà 50,257 √ó 768 ‚âà 39M
N_layer ‚âà 12 √ó 768¬≤ √ó 12 ‚âà 85M
N_total ‚âà 124M  (close to 117M quoted!)
```

**The paper's key finding:** The **ratio** of attention to FFN parameters doesn't matter‚Äîonly total N matters!

---

### Why Does Model Size (N) Matter More Than Architecture?

**Hypothesis 1: Capacity Theory**

More parameters = more "representational capacity" to store patterns.

**Analogy:** Think of the model as a lookup table:
- 100M params can store ~100M patterns
- 100B params can store ~100B patterns

Language has **immense** complexity (grammar, semantics, world knowledge). Larger tables ‚Üí better compression.

---

**Hypothesis 2: Lottery Ticket Hypothesis**

Larger models contain more **subnetworks**, increasing the chance that one of them is a "winning" configuration.

**Evidence:**
- Pruning large models often works better than training small models from scratch
- Suggests that **overparameterization helps optimization**, not just capacity

---

**Hypothesis 3: Optimization Landscape**

Larger models have **smoother loss landscapes** (fewer local minima).

**Evidence:**
- Wide networks are easier to optimize (see NTK theory)
- The paper observes that larger models require **fewer steps** to reach a given loss

---

### Width vs. Depth: Does It Matter?

**The Paper's Answer:** Not really!

**Experiment (Figure 2):** Fix N, vary (L, d_model):

| Configuration | L (layers) | d_model (width) | N (params) | Loss |
|---------------|-----------|-----------------|-----------|------|
| **Wide & Shallow** | 6 | 2048 | 110M | 3.15 |
| **Balanced** | 12 | 1024 | 110M | 3.14 |
| **Narrow & Deep** | 48 | 512 | 110M | 3.16 |

**Conclusion:** All three configurations perform nearly identically because N is the same.

**But there are subtleties:**

1. **Extremely shallow models** (L < 4) do perform worse‚Äîyou need *some* depth for hierarchical representations

2. **Extremely deep models** (L > 100) are **harder to optimize** (vanishing gradients, even with residual connections)

3. **Practical sweet spot:** L ‚âà ‚àöN is a heuristic that balances depth and width

---

### Attention Heads: Diminishing Returns

**The Paper's Finding:** Number of attention heads has **minimal effect** on performance.

**Experiment:** Fix N, vary number of heads:

```
1 head:  L = 3.20 nats
4 heads: L = 3.14 nats
16 heads: L = 3.12 nats
32 heads: L = 3.12 nats (no improvement!)
```

**Interpretation:**
- **More heads = more parallelism** for capturing different attention patterns (e.g., syntactic vs. semantic)
- But beyond ~16 heads, you hit diminishing returns‚Äîthe model can't use extra capacity

**Practical advice:** Use 16-32 heads (standard in GPT-3/4), don't obsess over this hyperparameter.

---

### Position Embeddings: Learned vs. Sinusoidal

**The Paper's Finding:** Doesn't matter for scaling laws!

**Options:**
1. **Learned embeddings:** Each position gets a trainable vector
2. **Sinusoidal (Attention Is All You Need):** Fixed sin/cos functions
3. **Relative positional encodings (T5):** Encode distances, not absolute positions
4. **Rotary embeddings (RoFormer):** Rotate Q/K based on position

**Performance:** All achieve similar loss at fixed N.

**Why?** The model has **enough capacity** to learn position implicitly through attention patterns. Explicit encodings just make training slightly faster/easier.

---

## üìä Visualizations & Key Tables

### Table 1: Scaling Law Comparison

| Law | Formula | Exponent | 10x Resource ‚Üí | 100x Resource ‚Üí |
|-----|---------|----------|----------------|-----------------|
| **L(N)** | (N_c/N)^0.076 | Œ±_N ‚âà 0.076 | 5% loss reduction | 10% loss reduction |
| **L(D)** | (D_c/D)^0.095 | Œ±_D ‚âà 0.095 | 6% loss reduction | 12% loss reduction |
| **L(C)** | (C_c/C)^0.050 | Œ±_C ‚âà 0.050 | 3% loss reduction | 6% loss reduction |

**Key insight:** Data scaling (Œ±_D = 0.095) is **more efficient** than parameter scaling (Œ±_N = 0.076), which is more efficient than compute scaling (Œ±_C = 0.050).

---

### Table 2: Optimal Allocation Examples

Given compute budget C, here's the optimal (N, D) allocation:

| Compute Budget C | Optimal N (params) | Optimal D (tokens) | Expected Loss |
|------------------|-------------------|-------------------|---------------|
| **0.001 PF-days** | 10M | 100M | 4.2 nats |
| **0.01 PF-days** | 50M | 300M | 3.5 nats |
| **0.1 PF-days** | 250M | 1B | 2.9 nats |
| **1.0 PF-days** | 1.3B | 3B | 2.4 nats |
| **10 PF-days** | 6B | 10B | 2.0 nats |
| **100 PF-days** | 30B | 30B | 1.7 nats |

*(Values approximate, based on formulas from Section 5)*

---

### Table 3: Historical Model Comparison

| Model | Year | N | D | Compute (PF-days) | Trained to convergence? | Loss (approx) |
|-------|------|---|---|------------------|------------------------|---------------|
| BERT-Large | 2018 | 340M | 3.3B | ~0.01 | ‚úì Yes | 3.3 |
| GPT-2 | 2019 | 1.5B | 40B | ~0.2 | ‚úì Yes | 2.9 |
| T5-11B | 2020 | 11B | 1T | ~5 | ‚úì Yes | 2.3 |
| GPT-3 | 2020 | 175B | 300B | ~300 | ‚úó No (early stop) | 2.0 |
| Gopher | 2021 | 280B | 300B | ~500 | ‚úó No (early stop) | 1.95 |
| Chinchilla | 2022 | 70B | 1.4T | ~500 | ‚úó No (early stop) | 1.85 |
| LLaMA-65B | 2023 | 65B | 1.4T | ~500 | ‚úó No (early stop) | 1.83 |

**Trend:** Over time, models shifted from training small models to convergence ‚Üí training large models with early stopping.

---

### Figure 1: Scaling Law Schematic (Conceptual)

```
Log(Loss)
    ‚Üë
  4.0|
     |    ‚óè                    L(N): Slope = -0.076
  3.5|      ‚óè‚óè                 L(D): Slope = -0.095
     |        ‚óè‚óè               L(C): Slope = -0.050
  3.0|          ‚óè‚óè
     |            ‚óè‚óè           All are straight lines
  2.5|              ‚óè‚óè         on log-log scale!
     |                ‚óè‚óè
  2.0|                  ‚óè‚óè
     |                    ‚óè‚óè
  1.5|____________________‚óè‚óè___|‚Üí Log(Resource)
     10‚Å∂  10‚Å∑  10‚Å∏  10‚Åπ  10¬π‚Å∞  10¬π¬π
```

**What this shows:** Loss decreases as a power law (straight line on log-log axes) across **7 orders of magnitude** of scale.

---

### Figure 2: Compute-Optimal Frontier

```
           Undertrained
              ‚Üó (too much N, not enough D)
             /
Optimal ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (balanced N and D)
             \
              ‚Üò Overtrained
           (too much D, not enough N)

Example:
  GPT-3 (175B, 300B):   Near optimal frontier
  T5 (11B, 1T):         Overtrained (could have used larger model)
  Gopher (280B, 300B):  Undertrained (needed more data)
```

---

## üìö Resources & Further Reading

### Primary Paper
- **Kaplan et al., "Scaling Laws for Neural Language Models" (2020)**
  [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

### Follow-Up Work

1. **Chinchilla Paper (DeepMind, 2022)**
   "Training Compute-Optimal Large Language Models"
   [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)
   **Key contribution:** Refined the scaling laws, showed GPT-3 was undertrained on data

2. **Hoffmann et al., "An Empirical Analysis of Compute-Optimal Training" (2022)**
   [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)
   **Key contribution:** Systematic IsoFLOP analysis, optimal N ~ D^0.5 (not N ~ D^0.74)

3. **Muennighoff et al., "Scaling Data-Constrained Language Models" (2023)**
   [arXiv:2305.16264](https://arxiv.org/abs/2305.16264)
   **Key contribution:** What to do when you run out of data? (repeated data, synthetic data)

---

### Related Theoretical Work

4. **Bahri et al., "Explaining Neural Scaling Laws" (2021)**
   [arXiv:2102.06701](https://arxiv.org/abs/2102.06701)
   **Key contribution:** Theoretical analysis of why power laws emerge (statistical mechanics perspective)

5. **Henighan et al., "Scaling Laws for Autoregressive Generative Modeling" (2020)**
   [arXiv:2010.14701](https://arxiv.org/abs/2010.14701)
   **Key contribution:** Extends scaling laws to images, video, and math (Transformers are universal!)

6. **Phuong & Hutter, "Formal Algorithms for Transformers" (2022)**
   [arXiv:2207.09238](https://arxiv.org/abs/2207.09238)
   **Key contribution:** Precise mathematical description of Transformer operations (included in this repo)

---

### Practical Guides

7. **EleutherAI Scaling Laws Blog Post**
   [https://blog.eleuther.ai/scaling-laws/](https://blog.eleuther.ai/scaling-laws/)
   **Key contribution:** Practitioner-friendly explanation with code examples

8. **Weights & Biases Scaling Laws Report**
   [https://wandb.ai/wandb_fc/articles/reports/Scaling-Laws-for-Neural-Language-Models--VmlldzoyMDg5Nzg1](https://wandb.ai/wandb_fc/articles/reports/Scaling-Laws-for-Neural-Language-Models--VmlldzoyMDg5Nzg1)
   **Key contribution:** Interactive visualizations and experiment tracking tips

---

### Critical Perspectives

9. **Schaeffer et al., "Are Emergent Abilities a Mirage?" (2023)**
   [arXiv:2304.15004](https://arxiv.org/abs/2304.15004)
   **Key contribution:** Challenges the "emergent abilities" narrative, argues scaling is smooth (not discontinuous)

10. **Wei et al., "Emergent Abilities of Large Language Models" (2022)**
    [arXiv:2206.07682](https://arxiv.org/abs/2206.07682)
    **Key contribution:** Counter-argument: some abilities DO emerge suddenly at scale

---

### Broader Context

11. **Sutton, "The Bitter Lesson" (2019)**
    [http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
    **Key contribution:** General methods that leverage computation win in the long run

12. **Thompson et al., "The Computational Limits of Deep Learning" (2020)**
    [arXiv:2007.05558](https://arxiv.org/abs/2007.05558)
    **Key contribution:** Environmental and economic limits of scaling (we can't scale forever!)

---

## üßë‚Äçüíª Python Code Examples

All code is available in the `/code` directory of this repository.

### Example 1: Predicting Loss from N and D

```python
# File: code/predict_loss.py
import numpy as np
import matplotlib.pyplot as plt

def predict_loss(N, D):
    """Predict test loss given model size and data size."""
    N_c = 8.8e13
    D_c = 5.4e13
    alpha_N = 0.076
    alpha_D = 0.095

    term_N = (N_c / N) ** (alpha_N / alpha_D)
    term_D = D_c / D
    L = (term_N + term_D) ** alpha_D
    return L

# Example: Vary N, fix D
N_values = np.logspace(6, 11, 50)  # 1M to 100B
D_fixed = 10e9  # 10B tokens

losses = [predict_loss(N, D_fixed) for N in N_values]

plt.figure(figsize=(10, 6))
plt.loglog(N_values, losses, linewidth=2)
plt.xlabel('Model Size N (parameters)', fontsize=14)
plt.ylabel('Test Loss (nats)', fontsize=14)
plt.title(f'Scaling Law: L(N) at D = {D_fixed/1e9:.0f}B tokens', fontsize=16)
plt.grid(True, alpha=0.3)
plt.show()
```

---

### Example 2: Compute-Optimal Allocation

```python
# File: code/optimal_allocation.py
import numpy as np
import pandas as pd

def compute_optimal_allocation(C):
    """Given compute budget C, return optimal (N, D)."""
    a = 0.73  # N scales as C^0.73
    b = 0.27  # D scales as C^0.27
    N_coeff = 0.3
    D_coeff = 3.2

    N_optimal = N_coeff * (C ** a) * 1e9  # Convert to raw params
    D_optimal = D_coeff * (C ** b) * 1e9  # Convert to raw tokens

    return N_optimal, D_optimal

# Generate table of allocations
C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]  # PF-days
results = []

for C in C_values:
    N, D = compute_optimal_allocation(C)
    L = predict_loss(N, D)
    results.append({
        'Compute (PF-days)': C,
        'Optimal N (params)': f'{N:.2e}',
        'Optimal D (tokens)': f'{D:.2e}',
        'Predicted Loss (nats)': f'{L:.2f}'
    })

df = pd.DataFrame(results)
print(df.to_string(index=False))
```

**Output:**
```
Compute (PF-days) Optimal N (params) Optimal D (tokens) Predicted Loss (nats)
              0.001           3.00e+07           3.20e+08                   4.20
              0.010           5.00e+07           4.50e+08                   3.50
              0.100           2.50e+08           1.00e+09                   2.90
              1.000           1.30e+09           3.00e+09                   2.40
             10.000           6.00e+09           1.00e+10                   2.00
            100.000           3.00e+10           3.00e+10                   1.70
```

---

### Example 3: Comparing Training Strategies

```python
# File: code/compare_strategies.py
import matplotlib.pyplot as plt
import numpy as np

def training_loss_curve(N, D, S_max):
    """Simulate loss curve during training."""
    # Loss decreases as model sees more data
    steps = np.linspace(1, S_max, 100)
    tokens_seen = steps * (2**19)  # Assume batch size of 524k

    # Effective data size grows with steps
    D_eff = np.minimum(tokens_seen, D)
    losses = [predict_loss(N, d) for d in D_eff]
    return steps, losses

# Compare three strategies with same compute budget
C = 1.0  # 1 PF-day

strategies = [
    ('Small model, long training', 100e6, 10e9, 50000),
    ('Medium model, medium training', 500e6, 5e9, 20000),
    ('Large model, short training', 2e9, 2e9, 5000),
]

plt.figure(figsize=(12, 7))
for name, N, D, S_max in strategies:
    steps, losses = training_loss_curve(N, D, S_max)
    plt.plot(steps, losses, label=f'{name}\nN={N/1e9:.1f}B, D={D/1e9:.0f}B', linewidth=2)

plt.xlabel('Training Steps', fontsize=14)
plt.ylabel('Test Loss (nats)', fontsize=14)
plt.title('Comparing Training Strategies (Fixed Compute Budget)', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()
```

---

### Example 4: Interactive Scaling Calculator

```python
# File: code/scaling_calculator.py

class ScalingCalculator:
    """Interactive calculator for scaling law predictions."""

    def __init__(self):
        self.N_c = 8.8e13
        self.D_c = 5.4e13
        self.alpha_N = 0.076
        self.alpha_D = 0.095
        self.alpha_C = 0.050

    def predict_loss(self, N=None, D=None, C=None):
        """Predict loss given any two of (N, D, C)."""
        if N is not None and D is not None:
            term_N = (self.N_c / N) ** (self.alpha_N / self.alpha_D)
            term_D = self.D_c / D
            L = (term_N + term_D) ** self.alpha_D
            return L
        elif C is not None:
            N_opt, D_opt = self.compute_optimal_allocation(C)
            return self.predict_loss(N=N_opt, D=D_opt)
        else:
            raise ValueError("Must provide either (N, D) or C")

    def compute_optimal_allocation(self, C):
        """Compute optimal (N, D) for budget C."""
        N_opt = 0.3 * (C ** 0.73) * 1e9
        D_opt = 3.2 * (C ** 0.27) * 1e9
        return N_opt, D_opt

    def compare_models(self, model_specs):
        """Compare multiple model configurations."""
        results = []
        for name, N, D in model_specs:
            L = self.predict_loss(N=N, D=D)
            results.append((name, N, D, L))
        return results

# Example usage
calc = ScalingCalculator()

# Predict GPT-3 performance
N_gpt3 = 175e9
D_gpt3 = 300e9
print(f"GPT-3 predicted loss: {calc.predict_loss(N=N_gpt3, D=D_gpt3):.3f} nats")

# Compare historical models
models = [
    ("GPT-2", 1.5e9, 40e9),
    ("GPT-3", 175e9, 300e9),
    ("Gopher", 280e9, 300e9),
    ("Chinchilla", 70e9, 1.4e12),
]

print("\nModel Comparison:")
for name, N, D, L in calc.compare_models(models):
    print(f"  {name:12} N={N/1e9:6.1f}B  D={D/1e9:8.0f}B  ‚Üí  L={L:.3f} nats")
```

---

## üéØ Key Takeaways for Practitioners

If you remember **nothing else** from this paper, remember these three points:

### 1. **Scale is Predictable**
Performance improves as a **power law** with model size, data size, and compute. You can **forecast** performance before spending millions on training.

### 2. **Bigger Models Are More Sample-Efficient**
Train **large models briefly** rather than small models to convergence. The optimal allocation: N grows much faster than D as compute increases.

### 3. **Architecture Details Matter Less Than You Think**
At fixed parameter count N, depth/width/heads have minimal impact. Focus on **scale**, not architecture search.

---

## üôè Acknowledgments

**Paper Authors:** Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei

**This Presentation:** Created for DS 5690 (Generative AI Models in Theory & Practice) at Vanderbilt University

**Repo Contents:**
- `papers/scaling_laws_paper.pdf` - Original paper
- `papers/rubric.pdf` - Presentation rubric
- `papers/formal_algorithms_transformers.pdf` - Reference on Transformer algorithms
- `code/` - Python implementations of scaling law algorithms

---

## üìñ Citation

If you reference this work, please cite the original paper:

```bibtex
@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
```

---

**Questions? Comments? Found an error?**
Open an issue on this repository or contact [your-email] (Github: Kshetkar1)

---

*Last updated: 2024*

